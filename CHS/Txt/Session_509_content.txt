大家好
我是Henry Mason是Siri语音识别工程师
今天我们非常激动地发布一项全新的API
它将让语音识别也能为你的app解决问题
先快速回顾一下什么是语音识别
语音识别是自动的过程
将人类语音的音频转换成文本
它取决于语音的语言
比如 英语会和汉语的识别不同
在iOS 大多数人会想到Siri
但语音识别对许多其他任务也有用
由于Siri与iPhone 4S一起发布
iOS也带有keyboard听写
在iOS keyboard空格键旁那个小小的麦克风按键
触发对任何UI kit文本输入的语音识别
每天有成千上万个应用使用这个功能
事实上 大约三分之一的请求来自第三方应用
它使用起来极其方便
它处理录音和录音中断
它显示用户界面
它不需要你再写任何代码
就能支持任何文本输入
而且它从iOS 5开始就可供使用
iOS keyboard听写从2011年起便可供使用
但它的简化带来很多限制
你的用户界面通常并不需要keyboard
当录音开始时你不能控制
不能控制使用哪一种语言
只是刚好使用系统的keyboard语言
甚至没有办法知道听写键是否可用
默认录音可能对你的使用案例不合理
你可能想要更多信息 而不只是文本
那么现在在iOS 10
我们引入一种新的语音框架
语音识别API更加强大
它使用相同基本技术和Siri及Dictation中所使用的一样
它提供快速而准确的结果
显而易见地定制给用户
而无需你收集任何用户数据
该框架也提供了识别的更多信息
而不只是文本
例如 我们也提供另外的解读
关于你的用户可能说了什么置信水平 以及定时信息
用于API的音频可来自预录文件
或现场来源 比如麦克风
语音识别API的可用性深远而广泛 经过许可
iOS 10支持超过50种语言和方言 从阿拉伯语到越南语
任何运行iOS 10的设备都支持
语音识别API通常能胜任
在需要互联网连接的大型服务器上
不过 某些新的设备确实时刻都支持语音识别
我们提供可用性API以确定
某个既定语言当前是否可用
使用这个 而不是去寻找互联网连接
由于语音识别需要
传送用户的音频经过互联网
用户必须明确提供许可给你的应用
在可以使用语音识别之前
语音识别解释、授权、请求
有四个主要步骤在你的应用中采用语音识别
首先在应用的Info.plist中提供使用描述
例如 你的相机应用Phromage
可能用了语音识别的使用描述...
这能让你只说cheese就能拍照
其次 请求授权利用请求授权级别方法
你先前提供的解释会被呈现给用户
在一个熟悉的对话中
然后用户将能够决定
他们是否想要让你的应用语音识别
接下来 创建语音识别请求
如果你已经有录好的音频文件
使用SFSpeechURLRecognitionRequest级别
否则 你要使用
SFSpeechAudioBufferRecognitionRequest
最后 提交识别请求
给SFSpeech Recognizer开始识别
你可以选择保留返回的识别任务
这有助于监控识别过程
我们来看看这个在代码中长什么样
假定我们已更新info.plist
通过准确的描述关于如何使用它
下一步是请求授权
也许最好等到
用户调用你的应用的功能后再这样做
这个功能要依靠语音识别
请求授权级别方法借助完成处理程序
它不保证某个执行语境
应用通常要发送到主队列
如果它们要做点什么比如开启或关闭用户界面按钮
如果你的授权处理程序已给出authorized状态
你应该准备开始识别
否则 识别就无法对你的应用可用
重要的是采用合适的方法禁用必要的功能
当用户作出这个决定时
或当设备受限 无法使用语音识别时
授权可稍后修改在设备的隐私设置里
我们来看看如何识别一个预录的音频文件
假设我们已有一个文件url
识别需要语音识别程序它只识别一种语言
默认的SFSpeechRecognizer启动程序可能会失败
于是我返回0如果区域不支持的话
默认的启动程序使用设备的当前区域
在这个功能中我们只要返回1 在这个情况下
虽然这个语音识别可能受支持但它也许不可用
可能由于没有互联网连接
使用isAvailable属性在你的识别程序中 以便监控它
现在我们创建一个识别请求用录好的文件的url
然后将它给予识别程序的识别任务方法
这个方法完成处理程序
借助两种可选的参数result和error
如果result是0
那意味着出于某种原因 识别失败
检查error的参数 寻求解释
否则 我们可以读出我们已经识别的语音
通过查看结果
注意 完成处理程序可能会被唤起不止一次
当语音被逐步识别
你可以确定识别已完成
通过检查结果的isFinal属性
这里我们只打印出最终识别的文本
识别来自设备麦克风的现场音频也很相似
但需要一些改动
我们要做出音频缓冲识别请求
这能让我们提供内存音频缓冲的序列
而不是硬盘上的文件
我们使用AVAudioEngine来获取音频缓冲流
然后将其附加到请求
注意 完全可以附加音频缓冲
到识别请求在开始识别之前和之后
一个不同之处在于
我们不再忽略识别任务方法的返回值
反而 我们要将它保存在一个变量的属性中
等会儿我们就知道为什么
当我们完成录音后
我们需要通知请求没有更多音频了
以便它能完成识别
使用endAudio方法来实现
但要是用户取消录音或者录音被中断呢？
在这种情况下 我们真的不关心结果
而且我们应该释放仍在被语音识别使用的任何资源
只要取消我们开始的识别任务...
我们开始识别时保存的
这对于预录音频的识别也能做到
最佳做法
简单说说一些最佳做法
资源负责任
我们开放语音识别给所有应用免费使用
但我们的确有设置一些合理的限制
以便这项服务一直对每个人可用
不同的设备可能受限于
每天可以识别的量
应用也会在全球范围内被节流根据每天的请求
正如API支持的其他服务例如CLGO Coder
要有所准备以处理网络和速率受限的故障
如果你发现你经常达到节流的限制
请告诉我们
同样重要的是 要注意语音识别
会极大地耗费电池和网络流量
对于iOS 10我们开始限制音频长度为大约一分钟
类似于keyboard听写的时长
隐私和可用性透明度
简单说说关于透明度以及尊重用户的隐私
如果你在录用户的语音
最好在你的用户界面中说得非常明确
播放录制的声音和/或显示可见的录制指示
可让用户清楚知道他们正在被录音
有些语音不适合识别
密码、健康数据、财务信息以及其他敏感语音
不应给予语音识别
显示识别的语音像Siri和Dictation做的
也能帮助用户理解你的应用在做什么
它对用户很有帮助
以便他们可以在识别出错时及时看到
总结
那么 开发者们
你们的应用现在可以免费获得
高性能的语音识别可识别几十种语言
但重要的是要得体地处理当它不可用时的情况
或者用户不想让你的应用使用它
透明度是最好的政策
让用户清楚知道什么时候语音识别正在被使用
我们很兴奋地期待你们会为语音识别带来什么新用途
更多信息
欲了解更多信息及一些样本代码请查看本讲的网页
你可能会对部分关于SiriKit的会话感兴趣
周三有一场周四有一场更高级别的
谢谢参与祝你们在 WWDC 大有收获